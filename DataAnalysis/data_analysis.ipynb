{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation & Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# After extracting the data from Amazon using scrapy, we have stored the data into a simple json text file.\n",
    "# Importing the data from the json text file into a pandas dataframe:\n",
    "amazon_data = pd.read_json('/home/adelo/1-system/1-disco_local/1-mis_archivos/1-pe/1-ciencia/1-computer_science_an_IT/2-data_science/1-Amazon_Laptops_Dashboard/amazon_data.json')\n",
    "\n",
    "# my_amazon_data[['ASIN','price','average_customer_reviews','number_reviews','number_ratings','tech_details','reviews']]\n",
    "amazon_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function takes a numeric string (<class 'str'>), removes any comma or dollar characters (\",\" \"$\") and\n",
    "# returns a numeric float value (<class 'float'>):\n",
    "def format_cleaner(val):\n",
    "    return float(val.replace(',','').replace('$',''))\n",
    "\n",
    "# After loading the data from the json file, every «review» entry is a dictionary type value that is composed of several fields: customer name, rating, date, title, and the text of the review itself.\n",
    "# Here we extract the relevant details (title and the text of the review itself) and create 3 new \n",
    "# columns to facilitate the handling of the «review» entries. We create the following columns: «review_title», «review_text» and «review_one_string»:\n",
    "number_of_reviews = []\n",
    "review_title      = []\n",
    "review_text       = []\n",
    "review_title_text = []\n",
    "review_one_string = []\n",
    "review_rating     = []\n",
    "\n",
    "for i in range(amazon_data.shape[0]):\n",
    "    review_title_per_item      = []\n",
    "    review_text_per_item       = []\n",
    "    review_title_text_per_item = []\n",
    "    review_one_string_per_item = ''\n",
    "    review_rating_per_item     = []\n",
    "    for j in range(len(amazon_data['reviews'][i])):\n",
    "        title  = amazon_data['reviews'][i][j]['title']\n",
    "        text   = amazon_data['reviews'][i][j]['review_text']\n",
    "        title_text = title+' '+text\n",
    "\n",
    "        rating = amazon_data['reviews'][i][j]['rating']\n",
    "        rating = rating.split()\n",
    "        rating = format_cleaner(rating[0])\n",
    "\n",
    "        review_title_per_item.append(title)\n",
    "        review_text_per_item.append(text)\n",
    "\n",
    "        review_title_text_per_item.append(title_text)\n",
    "\n",
    "        review_one_string_per_item += title+' '+text+' '\n",
    "        review_rating_per_item.append(rating)\n",
    "\n",
    "    number_of_reviews.append(j+1)\n",
    "    review_title.append(review_title_per_item)\n",
    "    review_text.append(review_text_per_item)\n",
    "\n",
    "    review_title_text.append(review_title_text_per_item)\n",
    "\n",
    "    review_one_string.append(review_one_string_per_item.rstrip())\n",
    "    review_rating.append(review_rating_per_item)\n",
    "\n",
    "amazon_data['number_of_reviews'] = number_of_reviews\n",
    "amazon_data['review_title']      = review_title\n",
    "amazon_data['review_text']       = review_text\n",
    "amazon_data['review_title_text'] = review_title_text\n",
    "amazon_data['review_one_string'] = review_one_string\n",
    "amazon_data['review_rating']     = review_rating\n",
    "\n",
    "\n",
    "# Here we make sure that the first character of the brand name is uppercase and \n",
    "# remaining characters lowercase. This is important because we are going to perform\n",
    "# filtering and searching function using the brand name so we need to make sure \n",
    "# that the writing is consistent.\n",
    "amazon_data['brand'] = [ amazon_data['tech_details'][i]['Brand Name'].title()  if   amazon_data['tech_details'][i]['Brand Name'] not in ['HP','hp','Hp']  else  amazon_data['tech_details'][i]['Brand Name'].upper()  for  i  in range(amazon_data.shape[0]) ]\n",
    "\n",
    "# After loading the data from the json file, all technical details are in a dictionary type entry.\n",
    "# In the following block we are extracting the tech details that are important for our analysis («series» and «model_number») and creating new columns for each of these relevant tech details\n",
    "# Series:\n",
    "amazon_data['series'] = [ amazon_data['tech_details'][i]['Series']  for  i  in range(amazon_data.shape[0]) ]\n",
    "# Model number:\n",
    "amazon_data['model_number'] = [ amazon_data['tech_details'][i]['Item model number']   for  i  in range(amazon_data.shape[0]) ]\n",
    "\n",
    "# After extracting the data from the web page, the numeric values (\"average_customer_reviews\" and \"price\") are actually of «string» type. So, We need to convert the entry to a numeric type (Float). This is necessary because we will perform mathematical operations with these values:\n",
    "\n",
    "# A raw «average_customer_reviews» entry looks like this: \"4.5 out of 5 stars\"  (<class 'str'>)\n",
    "# We only need the firs value as a numeric float type: 4.5  (<class 'float'>)\n",
    "# This is done in the next line of code over the entire dataframe by selecting only the \n",
    "# firs element (\"4.5\" in the above example) and applying the «format_cleaner()» function to the «average\\_customer\\_reviews» column:\n",
    "amazon_data['average_customer_reviews'] = [ format_cleaner(val[0]) for val in amazon_data['average_customer_reviews'].str.split() ]\n",
    "\n",
    "# A raw «price» entry looks like this: \"$689.90\"  (<class 'str'>)\n",
    "# We only need the numeric value: 689.90  (<class 'float'>)\n",
    "# This is done in the next line of code over the entire dataframe by applying the «format_cleaner()» function to the «price» column:\n",
    "amazon_data['price'] = amazon_data['price'].apply(lambda val: round(format_cleaner(val)) if pd.notnull(val) else val)\n",
    "amazon_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing\n",
    "This function allows performing the pre-processing directly over the «amazon_data» dataframe. Remembar that, in this dataframe, the reviews for a particular series are in a list that is inside the dataframe. The «pre-processing()» funtion included in «data_analysis2.ipynb» is, on the other hand, disigned to performe the pre-processing over the «reviews» dataframe\n",
    "\n",
    "* **Removing punctuation:**\n",
    " * Punctuation: We will remove all punctuation char found the «string» library.\n",
    " \n",
    "* **Removing stopwords:**\n",
    " * Our stopwords will be composed by:\n",
    "  - The common stopwords defined in the nltk library \n",
    "  - Some particular stopwords related to our data:\n",
    "    * Brand names: There is no point in analyzing brand names. For instance, in a Lenovo review, the customer will use the word ``Lenovo'' many times, but this fact does not contribute anything to the analysis. \n",
    "    * Laptop synonyms: laptop, computer, machine, etc.\n",
    "    * Some no-official contractions that are not in the nltk library: Im dont Ive, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "nltk.data.path.append('/home/adelo/.nltk/nltk_data')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Defining our stopwords list:\n",
    "stopwords_brands = [ b.lower() for b in list(set(amazon_data['brand'])) ]\n",
    "stopwords_brands_additionals = ['computer','computers','laptop','laptops','thing','things','machine','machines','im','dont','ive']\n",
    "stopwords_total  = stopwords.words('english') + stopwords_brands + stopwords_brands_additionals\n",
    "\n",
    "# The following function takes a string and returns the same string without punctuation or stopwords:\n",
    "def pre_processing(texto):\n",
    "    # Removing punctuation:\n",
    "    nopunct = ''.join([ char for char in texto if char not in string.punctuation ])\n",
    "    # Removing Stopwords:\n",
    "    return ' '.join([ word for word in nopunct.split() if word.lower() not in stopwords_total ])\n",
    "\n",
    "# The following function takes a list of strings and remove punctuation and stopwords from each string in the list\n",
    "def pre_processing_lista(lista):\n",
    "    return [pre_processing(texto) for texto in lista]\n",
    "\n",
    "# Here is how we would apply the function «pre_processing()» to a column over the entire dataframe.\n",
    "# However, we won't do that in this stage because we need a raw text for the Sentiment Analysis\n",
    "# amazon_data['review_title']      = amazon_data['review_title'].apply(pre_processing_lista)\n",
    "# amazon_data['review_text']       = amazon_data['review_text'].apply(pre_processing_lista)\n",
    "# amazon_data['review_title_text'] = amazon_data['review_title_text'].apply(pre_processing_lista)\n",
    "amazon_data['review_one_string'] = amazon_data['review_one_string'].apply(pre_processing)\n",
    "\n",
    "amazon_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon-based Sentiment Analysis\n",
    "* We are performing a Lexicon-based Sentiment Analysis using two popular Python libraries: **TextBlob** and **Vader Sentiment**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first example using TextBlob and Vader Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis with TextBlod\n",
    "# ================================\n",
    "\n",
    "# https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis\n",
    "\n",
    "# The polarity score is a float within the range [-1.0, 1.0].\n",
    "# The subjectivity is a float within the range   [ 0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Example\n",
    "TextBlob('I am great but the impotant thing is that I am not undarstanding what I have to do but I am not loving it ok and it is really bad bad bad bad bad bad bad bad bad bad lov').sentiment\n",
    "TextBlob(amazon_data['reviews_one_string'].loc[1]).sentiment\n",
    "# We can create a TextBlob object\n",
    "lista = ['hola','como','estas']\n",
    "analis = TextBlob(lista)\n",
    "analisis = TextBlob(\"TextBlob sure looks like it has some interesting features\")\n",
    "# TextBlob is not only a sentiment analysis library. Check all the methods a TextBlob object has\n",
    "# print(dir(analisis))\n",
    "# For example, we can use it for translation:\n",
    "# print(analisis.translate(to='es'))\n",
    "# We can also do Part-of-speech tagging\n",
    "# nltk.data.path.append('/home/adelo/.nltk/nltk_data')\n",
    "print(analisis.tags)\n",
    "# Now let's do our sentiment analysis\n",
    "print(analisis.sentiment)\n",
    "\n",
    "\n",
    "\n",
    "# Sentiment Analysis with Vader Sentiment\n",
    "# =======================================\n",
    "\n",
    "# https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f\n",
    "# https://pypi.org/project/vaderSentiment/\n",
    "# https://github.com/cjhutto/vaderSentiment\n",
    "# https://www.researchgate.net/profile/Cj_Hutto/publication/275828927_VADER_A_Parsimonious_Rule-based_Model_for_Sentiment_Analysis_of_Social_Media_Text/links/554775be0cf26a7bf4d90840/VADER-A-Parsimonious-Rule-based-Model-for-Sentiment-Analysis-of-Social-Media-Text.pdf\n",
    "\n",
    "# Polarity_scores:\n",
    "    # The «Compound score» is a metric that calculates the sum of all the lexicon ratings which have been normalized between:\n",
    "    # [-1, 1]    -1 (most extreme negative) and +1 (most extreme positive).\n",
    "    #\n",
    "    # The «Positive», «Negative» and «Neutral» scores represent the proportion of text that falls in these categories.\n",
    "    # This means our sentence was rated as 67% Positive, 33% Neutral and 0% Negative. Hence all these should add up to 1.\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Example\n",
    "pola_scores = analyser.polarity_scores('I love to eat but sometimes it is better not to eat so much, love love love bad')\n",
    "# display(pola_scores)\n",
    "# pola_scores['compound']\n",
    "frase = 'I love to eat but sometimes it is better not to eat so much, love love love bad'\n",
    "analyser.polarity_scores(frase)['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the Sentiment Analysis over the entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "# This is the sentiment polarity and subjectivity calculated over all the customer reviews for a particular series\n",
    "amazon_data['avg_polarity_textblob']     = [ round(TextBlob(review).sentiment.polarity, 5)     for review in amazon_data['review_one_string'] ]\n",
    "amazon_data['avg_subjectivity_textblob'] = [ round(TextBlob(review).sentiment.subjectivity, 5) for review in amazon_data['review_one_string'] ]\n",
    "amazon_data['avg_polarity_vader']        = [ analyser.polarity_scores(review)['compound']      for review in amazon_data['review_one_string'] ]\n",
    "\n",
    "\n",
    "# «polarity_title_textblob» and «subjectivity_title_textblob» are «lists» that contain the sentiment polarity and subjectivity, respectively, for each customer review title\n",
    "polarity_title_textblob = []\n",
    "subjectivity_title_textblob = []\n",
    "polarity_title_vader = []\n",
    "for review_title_list in amazon_data['review_title']:\n",
    "    polarity_textblob = []\n",
    "    subjectivity_textblob = []\n",
    "    polarity_vader = []\n",
    "    for review_title in review_title_list:\n",
    "        polarity_textblob.append(round(TextBlob(review_title).sentiment.polarity, 5))\n",
    "        subjectivity_textblob.append(round(TextBlob(review_title).sentiment.subjectivity, 5))\n",
    "        polarity_vader.append(analyser.polarity_scores(review_title)['compound'])\n",
    "    polarity_title_textblob.append(polarity_textblob)\n",
    "    subjectivity_title_textblob.append(subjectivity_textblob)\n",
    "    polarity_title_vader.append(polarity_vader)\n",
    "amazon_data['polarity_title_textblob'] = polarity_title_textblob\n",
    "amazon_data['subjectivity_title_textblob'] = subjectivity_title_textblob\n",
    "amazon_data['polarity_title_vader'] = polarity_title_vader\n",
    "\n",
    "\n",
    "# «polarity_text_textblob» and «subjectivity_text_textblob» are «lists» that contain the sentiment polarity and subjectivity, respectively, for each customer review (all the text of the customer review without the title)\n",
    "polarity_text_textblob = []\n",
    "subjectivity_text_textblob = []\n",
    "polarity_text_vader = []\n",
    "for review_text_list in amazon_data['review_text']:\n",
    "    polarity_textblob = []\n",
    "    subjectivity_textblob = []\n",
    "    polarity_vader = []\n",
    "    for review_text in review_text_list:\n",
    "        polarity_textblob.append(round(TextBlob(review_text).sentiment.polarity, 5))\n",
    "        subjectivity_textblob.append(round(TextBlob(review_text).sentiment.subjectivity, 5))\n",
    "        polarity_vader.append(analyser.polarity_scores(review_text)['compound'])\n",
    "    polarity_text_textblob.append(polarity_textblob)\n",
    "    subjectivity_text_textblob.append(subjectivity_textblob)\n",
    "    polarity_text_vader.append(polarity_vader)\n",
    "amazon_data['polarity_text_textblob'] = polarity_text_textblob\n",
    "amazon_data['subjectivity_text_textblob'] = subjectivity_text_textblob\n",
    "amazon_data['polarity_text_vader'] = polarity_text_vader\n",
    "\n",
    "\n",
    "# «polarity_title_textblob» and «subjectivity_title_textblob» are «lists» that contain the sentiment polarity and \n",
    "# subjectivity, respectively, for each customer review title\n",
    "polarity_title_text_textblob = []\n",
    "subjectivity_title_text_textblob = []\n",
    "polarity_title_text_vader = []\n",
    "length_title_text = []\n",
    "for review_title_text_list in amazon_data['review_title_text']:\n",
    "    polarity_textblob = []\n",
    "    subjectivity_textblob = []\n",
    "    polarity_vader = []\n",
    "    length_title_text_value = []\n",
    "    for review_title_text in review_title_text_list:\n",
    "        polarity_textblob.append(round(TextBlob(review_title_text).sentiment.polarity, 5))\n",
    "        subjectivity_textblob.append(round(TextBlob(review_title_text).sentiment.subjectivity, 5))\n",
    "        polarity_vader.append(analyser.polarity_scores(review_title_text)['compound'])\n",
    "        length_title_text_value.append(len(review_title_text))\n",
    "    polarity_title_text_textblob.append(polarity_textblob)\n",
    "    subjectivity_title_text_textblob.append(subjectivity_textblob)\n",
    "    polarity_title_text_vader.append(polarity_vader)\n",
    "    length_title_text.append(length_title_text_value)\n",
    "\n",
    "amazon_data['polarity_title_text_textblob'] = polarity_title_text_textblob\n",
    "amazon_data['subjectivity_title_text_textblob'] = subjectivity_title_text_textblob\n",
    "amazon_data['polarity_title_text_vader'] = polarity_title_text_vader\n",
    "amazon_data['length_title_text'] = length_title_text\n",
    "amazon_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion analysis using the NRC Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the NRC-Sentiment-Emotion-Lexicons\n",
    "filepath = ('NRC-Sentiment-Emotion-Lexicons/'\n",
    "            'NRC-Emotion-Lexicon-v0.92/'\n",
    "            'NRC-Emotion-Lexicon-Wordlevel-v0.92.txt')  \n",
    "lexiEmo_df0  = pd.read_csv(filepath,\n",
    "                        names=[\"word\", \"emotion\", \"association\"],\n",
    "                        sep='\\t')\n",
    "\n",
    "lexiEmo_df  = lexiEmo_df0.pivot(index='word',\n",
    "                                columns='emotion',\n",
    "                                values='association').reset_index()\n",
    "\n",
    "\n",
    "# This function returns a list that contains the emotions count in a text\n",
    "def getEmotionsDf(text, lexiEmo_df):\n",
    "    counterEmo = pd.Series(data=np.zeros(11).astype(int),index=lexiEmo_df.columns)\n",
    "    counterEmo.drop(index=['word'],inplace=True)\n",
    "\n",
    "    text_list = [word for word in text.split()]\n",
    "\n",
    "    for palabra in text_list:\n",
    "        if palabra in lexiEmo_df['word'].tolist():\n",
    "            i = lexiEmo_df.index[lexiEmo_df['word'] == palabra].tolist()\n",
    "            vectorEmo = lexiEmo_df.iloc[i[0]]\n",
    "            vectorEmo.drop(index=['word'],inplace=True)\n",
    "            counterEmo = counterEmo + vectorEmo\n",
    "\n",
    "    # If we want returning a dataframe:\n",
    "    # counterEmo_df = pd.DataFrame(counterEmo)\n",
    "    # counterEmo_df = counterEmo_df.rename(columns={0:'count'})\n",
    "    # counterEmo_df = counterEmo_df.reset_index()\n",
    "    # counterEmo_df = counterEmo_df.sort_values(by=['count'],ascending=True)\n",
    "    \n",
    "    # return counterEmo_df\n",
    "    return counterEmo.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the Emotion analysis over the entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = []\n",
    "for review_title_text_list in amazon_data['review_title_text']:\n",
    "    emotions_reviews = []\n",
    "    for review_title_text in review_title_text_list:\n",
    "        emotions_reviews.append(getEmotionsDf(review_title_text,lexiEmo_df))\n",
    "    emotions.append(emotions_reviews)\n",
    "\n",
    "amazon_data['emotions'] = emotions\n",
    "amazon_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the data in the disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting (in orden of importance) the most relevant columns for our analysis:\n",
    "cols = ['brand',\n",
    "        'series',\n",
    "        'model_number',\n",
    "        'price',\n",
    "        'number_of_reviews',\n",
    "        'review_title',\n",
    "        'review_text',\n",
    "        'review_title_text',\n",
    "        'review_one_string',\n",
    "        'average_customer_reviews',\n",
    "        'review_rating',\n",
    "        'avg_polarity_textblob',\n",
    "        'avg_subjectivity_textblob',\n",
    "        'avg_polarity_vader',\n",
    "        'polarity_title_textblob',\n",
    "        'subjectivity_title_textblob',\n",
    "        'polarity_title_vader',\n",
    "        'polarity_text_textblob',\n",
    "        'subjectivity_text_textblob',\n",
    "        'polarity_text_vader',\n",
    "        'polarity_title_text_textblob',\n",
    "        'subjectivity_title_text_textblob',\n",
    "        'polarity_title_text_vader',\n",
    "        'emotions',\n",
    "        'length_title_text']\n",
    "\n",
    "amazon_data = amazon_data[cols]\n",
    "display(amazon_data.number_of_reviews.sum())\n",
    "display(amazon_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_data.to_json(r'./data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the prepared and processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "amazon_data = pd.read_json('./amazon_data.json', precise_float=True)\n",
    "amazon_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/38895856/python-pandas-how-to-compile-all-lists-in-a-column-into-one-unique-list\n",
    "\n",
    "# This function returns a list\n",
    "def concaLists(serie_entry):\n",
    "        return [value for lista in serie_entry.tolist() for value in lista]\n",
    "\n",
    "brand, series, price  = [], [], []\n",
    "for  n  in  range(len(amazon_data)):\n",
    "    brand  += [amazon_data['brand'].iloc[n]]  * amazon_data['number_of_reviews'].iloc[n] \n",
    "    series += [amazon_data['series'].iloc[n]] * amazon_data['number_of_reviews'].iloc[n] \n",
    "    price  += [amazon_data['price'].iloc[n]]  * amazon_data['number_of_reviews'].iloc[n] \n",
    "\n",
    "# This function return a dataset with only review details with sentiment analysis\n",
    "def createSentimentDf(data):\n",
    "    return   pd.DataFrame({'title'                            : concaLists(data['review_title']),\n",
    "                           'text'                             : concaLists(data['review_text']),\n",
    "                           'title_text'                       : concaLists(data['review_title_text']),\n",
    "                           'rating'                           : concaLists(data['review_rating']),\n",
    "                           'polarity_title_textblob'          : concaLists(data['polarity_title_textblob']),\n",
    "                           'subjectivity_title_textblob'      : concaLists(data['subjectivity_title_textblob']),\n",
    "                           'polarity_title_vader'             : concaLists(data['polarity_title_vader']),\n",
    "                           'polarity_text_textblob'           : concaLists(data['polarity_text_textblob']),\n",
    "                           'subjectivity_text_textblob'       : concaLists(data['subjectivity_text_textblob']),\n",
    "                           'polarity_text_vader'              : concaLists(data['polarity_text_vader']),\n",
    "                           'polarity_title_text_textblob'     : concaLists(data['polarity_title_text_textblob']),\n",
    "                           'subjectivity_title_text_textblob' : concaLists(data['subjectivity_title_text_textblob']),\n",
    "                           'polarity_title_text_vader'        : concaLists(data['polarity_title_text_vader']),\n",
    "                           'emotions'                         : concaLists(data['emotions']),\n",
    "                           'length_title_text'                : concaLists(data['length_title_text']),\n",
    "                           'brand'                            : brand,\n",
    "                           'series'                           : series,\n",
    "                           'price'                            : price\n",
    "                           })\n",
    "\n",
    "my_reviews = createSentimentDf(amazon_data)\n",
    "my_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(amazon_data.groupby('brand').describe()['price']['mean'])\n",
    "display(amazon_data.groupby('brand').describe()['average_customer_reviews']['mean'])\n",
    "amazon_data.query('series in [\"Aspire E series\",\"L340 Gaming\"]')[['brand','series','price','average_customer_reviews']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_data[amazon_data['brand'] == 'Asus'][['brand','price','average_customer_reviews']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reviews_flex14 = ' '.join(amazon_data[amazon_data['series'] == 'Flex 14']['review_one_string'])\n",
    "theWords=['use','good','screen']\n",
    "for theWord in theWords:\n",
    "    count=0\n",
    "    for word in reviews_flex14.split():\n",
    "        if word == theWord:\n",
    "            count+=1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "992 260\n"
    }
   ],
   "source": [
    "acer_pos_reviews = my_reviews[(my_reviews['brand'] == 'Acer') & (my_reviews['rating'] > 3)]\n",
    "acer_neg_reviews = my_reviews[(my_reviews['brand'] == 'Acer') & (my_reviews['rating'] < 3)]\n",
    "print(len(acer_pos_reviews), len(acer_neg_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "count      992.000000\nmean       925.718750\nstd       1121.461209\nmin         14.000000\n25%        285.000000\n50%        627.500000\n75%       1141.000000\nmax      10490.000000\nName: length_title_text, dtype: float64"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "acer_pos_reviews.describe()['length_title_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda11de9112d84a49afbbf20f955bb49b93",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}